<!DOCTYPE html>
<html>
<title>DDP-Webpage</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<style>
body,h1,h2,h3,h4,h5 {font-family: "Raleway", sans-serif}

.bgimg {
  background-position: center;
  background-size: cover;
  background-image: url("images/surv3.jpg");
  min-height: 75%;
  opacity: 0.75;
}

</style>
<body class="w3-light-grey">

<!-- w3-content defines a container for fixed size centered content, 
and is wrapped around the whole page content, except for the footer in this example -->


<!-- Header header class="bgimg w3-display-container"-->

<header>

<div class="w3-container w3-center w3-padding-64"> 
  <h1><b>Person Tracking and Person Re-identification : <br> Surveillance in Multi-Camera Systems</b></h1>
</div>

   
</header>
<div class="w3-content" style="max-width:1400px">
<!-- Grid -->
<div class="w3-row">

<!-- Blog entries -->
<div class="w3-col l9 s12">
  <!-- Blog entry -->
  <div class="w3-card-4 w3-margin w3-white">
    <div class="w3-container">
      <h3><b>Introduction</b></h3>
      <p class="c0"><span class="c4 c3"></span></p><p class="c2"><span class="c4 c3">A digital video surveillance system is a surveillance system capable of capturing images and videos that can be compressed, stored or sent over communication networks. Video surveillance can be used by governments for intelligence gathering, prevention of crime, the protection of a process, person, group or object, or the investigation of crime. It is also used by criminal organizations to plan and commit crimes, and by businesses to gather intelligence on their competitors, suppliers or customers. Though there are some disadvantages in certain applications, the field of surveillance has a great potential and hence is worth exploring.</span></p><p class="c0"><span class="c4 c3"></span></p><p class="c2"><span class="c4 c3">Here we will try to understand how to implement an end to end surveillance system with &lsquo;person&rsquo; or &lsquo;human&rsquo; as a central part of our focus. This involves multiple steps like:</span></p><ul class="c15 lst-kix_z1shf5d8la3f-0 start"><li class="c6 c8"><span class="c4 c3">Person Detection</span></li><li class="c6 c8"><span class="c4 c3">Person Tracking</span></li><li class="c6 c8"><span class="c4 c3">Person Re-identification (Person Re-ID)</span></li></ul><p class="c0"><span class="c4 c3"></span></p><p class="c6"><span class="c4 c3">Following is the basic pipeline of the process:</span></p>

<p class="c0"><span class="c4 c3"></span></p>
<img src="images/image2.png" alt="Nature" style="width:80%; display: block; margin-left: auto; margin-right: auto; title="Flow of Person Tracking and Re-ID"" >

<p class="c6" style="text-align:center;" ><span class="w3-opacity">Fig 1: Flow of Person Tracking and Re-ID</span></p><p class="c0"><span class="c4 c3"></span></p>

<p class="c2"><span class="c3">&lsquo;Transfer of Metadata&rsquo; step can be skipped if our central system and data capturing system is one and the same. For our case this holds true and hence is skipped in the following process. Further information on this would be available here: (</span><span class="c5 c3"><a class="c1" href="https://www.google.com/url?q=https://github.com/nikunjmshah/Person-Tracking-and-Re-ID&amp;sa=D&amp;ust=1590343048037000">Github link</a></span><span class="c4 c3">)</span></p><p class="c0"><span class="c4 c3"></span></p><p class="c6"><span class="c4 c3">Now let us consider each of the above steps one by one:</span></p>
    </div>
  </div>
  <hr>

  <!-- Blog entry -->
  <div class="w3-card-4 w3-margin w3-white">
    <div class="w3-container">
      <h3><b>Person Detection</b></h3>
<p class="c0"><span class="c4 c3"></span></p><p class="c6"><span class="c4 c3">Normally person detection can be performed by two different techniques:</span></p><ol class="c15 lst-kix_lv7aojtcw5i6-0 start" start="1"><li class="c6 c8"><span class="c4 c3">Machine Learning (ML) based</span></li><li class="c6 c8"><span class="c4 c3">Non-ML based</span></li></ol><p class="c0"><span class="c4 c3"></span></p><p class="c2"><span class="c4 c3">Non-ML based algorithms are usually less accurate and require manual feature extraction, this becomes time-consuming and environment dependent in most cases. On the other hand in ML based techniques using neural networks it is possible to achieve human level accuracy along with decent speed. Hence we proceed with ML based person detection.</span></p><p class="c0"><span class="c4 c3"></span></p><p class="c2"><span class="c4 c3">In neural networks based algorithms we specifically use Single Shot Detection (SSD) algorithm which processes the complete image for only one time and gives output as the bounding box coordinates detected persons in the image. This results in faster person detection overall.</span></p><p class="c0"><span class="c4 c3"></span></p>

<h5 class="c7" id="h.816ooiyspbto"><b><span>Implementation details for Mobilenet-SSD:</span></b></h5><p class="c2"><span class="c3">For general image processing we use the popular library called &lsquo;OpenCV&rsquo; (</span><span class="c5 c3"><a class="c1" href="https://www.google.com/url?q=https://www.pyimagesearch.com/2018/05/28/ubuntu-18-04-how-to-install-opencv/&amp;sa=D&amp;ust=1590343048040000">Link for installation</a></span><span class="c3">). It has an inbuilt module named &lsquo;dnn&rsquo; which can implement neural networks given the model structure and weights. Same has been used to implement Mobilenet-SSD as follows: (</span><span class="c5 c3"><a class="c1" href="https://www.google.com/url?q=https://github.com/nikunjmshah/Person-Tracking-and-Re-ID/blob/master/people-counting/people_counter.py&amp;sa=D&amp;ust=1590343048041000">Source file</a></span><span class="c4 c3">)</span></p><ol class="c15 lst-kix_utw33nadvxik-0 start" start="1"><li class="c6 c8"><span class="c4 c3">Define model weights and parameters:</span></li></ol><p class="c6 c12"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 746.00px; height: 55.00px;"><img alt="" src="images/image10.png" style="width: 751.00px; height: 55.00px; margin-left: -5.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c12"><span class="c4 c3"></span></p><ol class="c15 lst-kix_utw33nadvxik-0" start="2"><li class="c6 c8"><span class="c4 c3">Load the model:</span></li></ol><p class="c6 c12"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 751.00px; height: 72.00px;"><img alt="" src="images/image3.png" style="width: 751.00px; height: 72.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c12"><span class="c4 c3"></span></p><ol class="c15 lst-kix_utw33nadvxik-0" start="3"><li class="c6 c8"><span class="c4 c3">Finally run the model to get output detection:</span></li></ol><p class="c6 c12"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 781.00px; height: 121.00px;"><img alt="" src="images/image8.png" style="width: 781.00px; height: 121.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c4 c3">&nbsp;</span></p><h5 class="c7" id="h.rzv5oknyjm4p"><span><b>Implementation details for Yolo (This is also SSD): </b></span></h5><p class="c2"><span class="c3">For yolo we need to clone the official repository from darknet (Reference: </span><span class="c5 c3"><a class="c1" href="https://www.google.com/url?q=https://github.com/pjreddie/darknet&amp;sa=D&amp;ust=1590343048043000">Darknet</a></span><span class="c3">). Same is also available </span><span class="c5 c3"><a class="c1" href="https://www.google.com/url?q=https://github.com/nikunjmshah/Person-Tracking-and-Re-ID/tree/master/darknet&amp;sa=D&amp;ust=1590343048043000">here</a></span><span class="c4 c3">&nbsp;with necessary modification and weights for our project.</span></p><p class="c0"><span class="c4 c3"></span></p><p class="c6"><span class="c4 c3">Once it is cloned we need to &lsquo;make&rsquo; the module after some modifications in Makefile as follows:</span></p><ul class="c15 lst-kix_hj7exgifwgtn-0 start"><li class="c2 c8"><span class="c4 c3">If we need to use GPU resources and have appropriate (Cuda) software installed, we can set GPU = 1</span></li><li class="c6 c8"><span class="c4 c3">Also set OPENMP = 1 to use process parallelization on CPU</span></li><li class="c6 c8"><span class="c4 c3">Set OPENCV = 1 to compile darknet with opencv (optional)</span></li></ul><p class="c2 c16"><span class="c4 c3"></span></p><p class="c2"><span class="c3">Once we have successfully executed the &lsquo;make&rsquo; command we can test and run &lsquo;yolo&rsquo; model for object detection. Note one would also need to download appropriate weights from </span><span class="c5 c3"><a class="c1" href="https://www.google.com/url?q=https://pjreddie.com/darknet/yolo/&amp;sa=D&amp;ust=1590343048045000">this</a></span><span class="c4 c3">&nbsp;link.</span></p><p class="c0"><span class="c4 c3"></span></p><p class="c6"><span class="c3">Some modifications are required in file python/darknet.py for successful execution of further algorithms. These are already made </span><span class="c5 c3"><a class="c1" href="https://www.google.com/url?q=https://github.com/nikunjmshah/Person-Tracking-and-Re-ID/blob/master/darknet/python/darknet.py&amp;sa=D&amp;ust=1590343048045000">here</a></span><span class="c4 c3">. One can save and replace the original file with this one.</span></p><p class="c0"><span class="c4 c3"></span></p>

<img alt="" src="images/image7.jpg" style="width: 281.66px; height: 212.50px; margin-left: 45px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Yolo">

<img alt="" src="images/image11.jpg" style="width: 281.66px; height: 212.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Tiny-Yolo"></span>

<img alt="" src="images/image1.jpg" style="width: 281.66px; height: 212.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Mobilenet-SSD">



<p class="c11" style="text-align:center;"><span class="w3-opacity">Fig 2: Comparison between Yolo, Tiny Yolo and Mobilenet-SSD</span></p><p class="c0">
<span class="c4 c3"></span></p><p class="c6"><span class="c4 c3">Key observations in above detections:</span></p><ul class="c15 lst-kix_qy6qaucrmlem-0 start"><li class="c2 c8"><span class="c4 c3">Accuracy wise &lsquo;Yolo&rsquo; performs the best. Disadvantage here is the algorithm is slow and hence leads to higher running time.</span></li><li class="c2 c8"><span class="c4 c3">&lsquo;Tiny Yolo&rsquo; is fastest among 3 but has some misclassification errors (like car/truck in above case).</span></li><li class="c2 c8"><span class="c3">&lsquo;Mobilenet-SSD&rsquo; has decent accuracy as well as less running time, but the algorithm is aggressive meaning multiple detections are shown for a single object in many cases. This can be removed to some extent by </span><span class="c5 c3"><a class="c1" href="https://www.google.com/url?q=https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c&amp;sa=D&amp;ust=1590343048047000">Non Maximum Suppression</a></span><span class="c4 c3">&nbsp;algorithm (which is already added to our final code).</span></li></ul><p class="c0"><span class="c4 c3"></span></p><p class="c2"><span class="c3">Note that all the following process of tracking and Re-ID is algorithmically independent of whichever detection model we use above. So one can use any of the models depending on his/her requirements and available resources. We have codes available for all three of them on our </span><span class="c5 c3"><a class="c1" href="https://www.google.com/url?q=https://github.com/nikunjmshah/Person-Tracking-and-Re-ID&amp;sa=D&amp;ust=1590343048048000">github</a></span><span class="c4 c3">&nbsp;repository.</span></p>
    </div> 
  </div>


<hr>
<!-- Blog entry -->
  <div class="w3-card-4 w3-margin w3-white">
    <div class="w3-container">
      <h3><b>Person Tracking</b></h3>
	<p class="c0"><span class="c4 c3"></span></p><p class="c2"><span class="c3">Next we move on to person tracking. In simpler words we need to track each person according to his movements, in each future frame, once he has entered the frame and until he disappears from the frame. This can be done by a feature matching algorithm like surf or sift. But we specifically use the &lsquo;dlib&rsquo; library which is common and quite accurate for such tracking tasks. (Reference: </span><span class="c5 c3"><a class="c1" href="https://www.google.com/url?q=https://www.pyimagesearch.com/2018/10/22/object-tracking-with-dlib/&amp;sa=D&amp;ust=1590343048048000">Pyimage Tutorial</a></span><span class="c4 c3">)</span></p><p class="c0"><span class="c4 c3"></span></p><p class="c6"><span class="c4 c3">Overall dlib based tracking works as follows:</span></p><ol class="c15 lst-kix_nz9zdqgvn2g6-0 start" start="1"><li class="c6 c8"><span class="c4 c3">Detect objects every &lsquo;N&rsquo; frames using appropriate detection algorithm</span></li><li class="c6 c8"><span class="c4 c3">Filter out the objects we need to track (for our case &lsquo;persons&rsquo;) from all detected objects</span></li><li class="c6 c8"><span class="c4 c3">Provide bounding box coordinates to &lsquo;dlib correlation tracker&rsquo;</span></li><li class="c6 c8"><span class="c4 c3">For new frame update the tracker by calling &lsquo;tracker.update()&rsquo; function</span></li><li class="c6 c8"><span class="c4 c3">Get positions of bounding box of our tracked object by function &lsquo;tracker.get_position()&rsquo;</span></li><li class="c6 c8"><span class="c4 c3">Update the image and repeat above procedure again</span></li></ol><p class="c0"><span class="c4 c3"></span></p><p class="c2"><span class="c3">The main &lsquo;</span><span class="c3">dlib correlation tracker&rsquo; implementation</span><span class="c3">&nbsp;is based on Danelljan et al.&rsquo;s 2014 paper,</span><span class="c3"><a class="c1" href="https://www.google.com/url?q=http://www.bmva.org/bmvc/2014/papers/paper038/index.html&amp;sa=D&amp;ust=1590343048050000">&nbsp;</a></span><span class="c5 c3"><a class="c1" href="https://www.google.com/url?q=http://www.bmva.org/bmvc/2014/papers/paper038/index.html&amp;sa=D&amp;ust=1590343048051000">Accurate Scale Estimation for Robust Visual Tracking</a></span><span class="c3">. This work is actually built on the popular MOSSE tracker from Bolme et al.&rsquo;s 2010 work,</span><span class="c3"><a class="c1" href="https://www.google.com/url?q=http://www.cs.colostate.edu/~vision/publications/bolme_cvpr10.pdf&amp;sa=D&amp;ust=1590343048051000">&nbsp;</a></span><span class="c5 c3"><a class="c1" href="https://www.google.com/url?q=http://www.cs.colostate.edu/~vision/publications/bolme_cvpr10.pdf&amp;sa=D&amp;ust=1590343048051000">Visual Object Tracking using Adaptive Correlation Filters</a></span><span class="c3">. W</span><span class="c4 c3">hile the MOSSE tracker works well for objects that are translated, it often fails for objects that change in scale. This is handled better by Danelljan&rsquo;s work mentioned earlier.</span></p><p class="c0"><span class="c4 c3"></span></p><p class="c2"><span class="c4 c3">Note that we do not require to run our detection algorithm again to only track the objects once they are detected, this helps us reduce our computation time as detection is a more heavy task compared to solely tracking. But on the other hand if we completely avoid detection in future frames we can actually miss the new ids (persons) who have entered into the frame after first detection. This is not acceptable. So we need to keep performing detection after every certain number of frames so that we do not miss a new entity. This idea is implemented in our code using a parameter called &lsquo;skipframes&rsquo;. It is exactly the number of frames after which the detection algorithm is called again. Ideal value of this parameter depends on our frame rate as well as camera view. &nbsp;</span></p><p class="c0"><span class="c4 c3"></span></p>


<img src="images/image5.gif" alt="Nature" style="width:70%; display: block; margin-left: auto; margin-right: auto; title="Tracking with dlib"" >

<p class="c6" style="text-align:center;" ><span class="w3-opacity">Fig 3: Tracking with dlib</span></p><p class="c0"><span class="c4 c3"></span></p>


    </div>
  </div>


<hr>
<!-- Blog entry -->
  <div class="w3-card-4 w3-margin w3-white">
    <div class="w3-container">
      <h3><b>Metadata Generation</b></h3>
	<p class="c0"><span class="c4 c3"></span></p><p class="c2"><span class="c4 c3">Metadata implies data extracted from surveillance videos in real time which seems to be important for further analysis. Following are some significant advantages of extracting and storing metadata:</span></p><ul class="c15 lst-kix_iejx8fddwzvx-0 start"><li class="c2 c8"><span class="c4 c3">Saves storage space as some videos of surveillance can be deleted (or not saved in the first place) considering their usefulness by preliminarily analysis</span></li><li class="c2 c8"><span class="c4 c3">Reduces computation time as well as amount of data to be crunched for further analysis as only important bits are stored</span></li><li class="c2 c8"><span class="c4 c3">Possibility of simultaneous analysis in case of multi-camera systems without any requirement of complete video recordings</span></li></ul><p class="c2 c16"><span class="c4 c3"></span></p><p class="c2"><span class="c4 c3">We store following metadata from our camera input livestream:</span></p><ol class="c15 lst-kix_jerswo87d4yv-0 start" start="1"><li class="c2 c8"><span class="c4 c3">One snapshot of each new person entering in the frame</span></li><li class="c2 c8"><span class="c4 c3">Frame number, center coordinates and movement direction of each ID</span></li><li class="c2 c8"><span class="c4 c3">Frame number and IDs when contact between two IDs is possible (for contact tracing)</span></li><li class="c2 c8"><span class="c4 c3">Count of individuals or IDs in each frame</span></li></ol><p class="c0"><span class="c4 c3"></span></p><p class="c2"><span class="c4 c3">More or less metadata can be stored according to requirement in future analysis. Metadata is primarily stored in &lsquo;txt&rsquo; format except for images to occupy minimal storage space.</span></p><p class="c2 c16"><span class="c4 c3"></span></p><p class="c2"><span class="c3">To store this metadata we need to append to appropriate text files when our code is running. One such example can be observed in this </span><span class="c5 c3"><a class="c1" href="https://www.google.com/url?q=https://github.com/nikunjmshah/Person-Tracking-and-Re-ID/blob/master/people-counting/people_counter_contact.py%23L230&amp;sa=D&amp;ust=1590343048055000">code</a></span><span class="c4 c3">&nbsp;as shown in figure below:</span></p><p class="c0"><span class="c4 c3"></span></p>


<img src="images/image9.png" alt="Nature" style="width:80%; display: block; margin-left: auto; margin-right: auto; title="Metadata generating code"" >


<p class="c11" style="text-align:center;"><span class="w3-opacity">Fig 4: Metadata generating code</span></p><p class="c0"><span class="c4 c3"></span></p><p class="c6"><span class="c4 c3">For storing images we have a special function in our tracker program to store the snapshot (using OpenCV) whenever a new ID comes in and is in visible range (i.e. approximately in the center of the frame). </span></p><p class="c0"><span class="c4 c3"></span></p><p class="c6"><span class="c4 c3">Examples of generated data are attached as follows:</span></p><ul class="c15 lst-kix_xhx31iuyrp7b-0 start"><li class="c6 c8"><span class="c5 c3"><a class="c1" href="https://www.google.com/url?q=https://github.com/nikunjmshah/Person-Tracking-and-Re-ID/tree/master/people-counting/gallery&amp;sa=D&amp;ust=1590343048057000">Snapshots</a></span></li><li class="c6 c8"><span class="c3 c5"><a class="c1" href="https://www.google.com/url?q=https://github.com/nikunjmshah/Person-Tracking-and-Re-ID/tree/master/people-counting/contacts&amp;sa=D&amp;ust=1590343048057000">Contact Tracing</a></span></li></ul>

    </div>
  </div>


<hr>
<!-- Blog entry -->
  <div class="w3-card-4 w3-margin w3-white">
    <div class="w3-container">
      <h3><b>Person Re-ID</b></h3>
	<p class="c0"><span class="c4 c3"></span></p><p class="c2"><span class="c4 c3">This is the last step in our human surveillance system pipeline and also the most important one. Person Re-ID basically involves finding an image similar (if it exists) to a given particular image (called as query image) from a group of images (called as gallery). </span></p><p class="c2 c16"><span class="c4 c3"></span></p><p class="c2"><span class="c4 c3">This is generally very useful in case of multi-camera systems. Suppose we have metadata of snapshots of each and every IDs observed in two different cameras or scenes. And we need to find out if any person observed in camera 1 / scene 1 is also present in camera 2 / scene 2 so that we don&rsquo;t consider them as two different persons. In this case the Person Re-ID algorithm can solve our problem. We can simply keep metadata of one of the cameras as our gallery and use every image of remaining metadata as our queries. This will find all the matching IDs (if any). This technique can be easily extended to more than two cameras and can also be used for a single camera (if we have recordings at different timestamps).</span></p><p class="c2 c16"><span class="c4 c3"></span></p>

<h5 class="c19" id="h.ndbj7x6ohp2l"><span><b>How is Re-ID done?</span></b></h5><p class="c2"><span class="c4 c3">Our Person Re-ID algorithm works roughly as follows:</span></p><ol class="c15 lst-kix_jyl7ux2yfmwb-0 start" start="1"><li class="c2 c8"><span class="c4 c3">Image classification network (Googlenet / resnet) with pretrained weights on Imagenet dataset is modified in the last layer. Number of outputs in the last layer are made equal to the number of distinct identities we have in our training data set.</span></li><li class="c2 c8"><span class="c4 c3">Last layer is re-trained from scratch keeping other layers fixed for certain epochs and the complete model is finetuned in later epochs. Basically we train the original image classification network to classify each image according to the person&#39;s ID in our training set.</span></li><li class="c2 c8"><span class="c4 c3">So doing the above steps we get a model which takes an image as an input and outputs a N-dimensional vector of probabilities (where N is no. of distinct identities in training data). This can be considered as a &lsquo;feature vector&rsquo; for that image.</span></li><li class="c2 c8"><span class="c4 c3">During testing we calculate the &lsquo;feature vector&rsquo; of our query image and try to match it with the &lsquo;feature vector&rsquo; of each of the gallery images using suitable similarity measures to identify if both of them belong to the same ID or not. This is indeed re-identification.</span></li></ol><p class="c0"><span class="c4 c3"></span></p>
<h5 class="c7" id="h.dqg4ue3fpdjz"><span class="c4 c17"><b>Re-ID Testing</span></b></h5><p class="c6"><span class="c4 c3">To test our Re-ID algorithm following pipeline is followed:</span></p>

<img src="images/image6.jpg" alt="Nature" style="width:80%; display: block; margin-left: auto; margin-right: auto; title="Re-ID Testing Pipeline"" >


<p class="c11" style="text-align:center;"><span class="w3-opacity">Fig 5: Re-ID Testing Pipeline</span></p><p class="c0"><span class="c4 c3"></span></p>
<h5 class="c7" id="h.6uustfcsn393"><b><span class="c4 c17">Other Details and Example</span></b></h5>
<p class="c6"><span class="c3 c20"><b>Dataset:</b></span><span class="c3">&nbsp;We use Market-1501 open source dataset available on the web (</span><span class="c5 c3"><a class="c1" href="https://www.google.com/url?q=https://www.aitribune.com/dataset/2018051063&amp;sa=D&amp;ust=1590343048061000">Link</a></span><span class="c4 c3">). It has 750 identities for training and 751 for testing. Any other suitable person re-id dataset can also be used.</span></p><p class="c0"><span class="c4 c3"></span></p><p class="c6"><span class="c3 c20"><b>Environment and code:</b></span><span class="c3">&nbsp;Pytorch was used as our primary ML library. Suitable modifications were made on &lsquo;</span><span class="c5 c3"><a class="c1" href="https://www.google.com/url?q=https://github.com/KaiyangZhou/deep-person-reid&amp;sa=D&amp;ust=1590343048062000">Torchreid</a></span><span class="c4 c3">&rsquo; library licensed by MIT. </span></p><p class="c6"><span class="c3">Our complete code for Person Re-ID is available on github - (</span><span class="c5 c3"><a class="c1" href="https://www.google.com/url?q=https://github.com/nikunjmshah/Person-Tracking-and-Re-ID/tree/master/deep-reid&amp;sa=D&amp;ust=1590343048063000">deep-reid</a></span><span class="c4 c3">)</span></p><p class="c0"><span class="c4 c3"></span></p><p class="c6"><span class="c3 c20"><b>Example:</b></span><span class="c4 c3">&nbsp;Output on Market1501 test image</span></p><p class="c0"><span class="c4 c3"></span></p>


<img src="images/image4.png" alt="Nature" style="width:80%; display: block; margin-left: auto; margin-right: auto; title="Top 5 matches of given query image in gallery image set"" >

<p class="c11" style="text-align:center;"><span class="w3-opacity">Fig 6: Top 5 matches of given query image in gallery image set</span></p><p class="c0"><span class="c4 c3"></span></p>

    </div>
  </div>	
<!-- END BLOG ENTRIES -->
</div>

<!-- Introduction menu -->
<div class="w3-col l3">
  <!-- About Card -->
  <div class="w3-card w3-margin w3-margin-top">
  <img src="images/me.PNG" style="width:100%">
    <div class="w3-container w3-white">
      <h4><b>About Me</b></h4>
      <p>Myself Nikunj Shah, I am a final year Dual Degree student in Mechanical Engineering at IIT Bombay. My specialization is in the subject Computer Integrated Manufacturing. My interests lie in the fields of Artificial Intelligence, Robotics and Automation. This blog presents the research & project work done as part of my Master's Project in my final year.</p>
    </div>
  </div><hr>

  <!-- Guide card -->
  <div class="w3-card w3-margin w3-margin-top">
    <div class="w3-container w3-white">
      <h4><b>Guide Professor</b></h4>
      <p>Prof. Asim Tewari, Mechanical Engineering, IIT Bombay</p>
      <p>Profile page: <a href="https://www.me.iitb.ac.in/?q=faculty/Prof.%20Asim%20Tewari">Faculty homepage</a> </p>
      <p>Email: asim.tewari[at]iitb.ac.in</p>
    </div>
  </div><hr>

  <!-- project det -->
  <div class="w3-card w3-margin w3-margin-top">
    <div class="w3-container w3-white">
      <h4><b>Project Repository</b></h4>
      <p class="c0"><span class="c4 c3"></span></p><p class="c6"><span class="c3">Github link for this Project: </span><br>
      <span class="c5 c3"><a class="c1" href="https://www.google.com/url?q=https://github.com/nikunjmshah/Person-Tracking-and-Re-ID&amp;sa=D&amp;ust=1590343048034000">Person-Tracking-and-Re-ID</a></span></p>
    </div>
  </div><hr>
  
  <!-- Contact -->
  <div class="w3-card w3-margin">
    <div class="w3-container w3-padding">
      <h5>Contact me?</h5>
    </div>
    <ul class="w3-ul w3-hoverable w3-white">
      <li class="w3-padding-16" ><a style="text-decoration:none" href="https://github.com/nikunjmshah">
        
        <span class="w3-large"><i class="fa fa-google" style="font-size:24px" ></i> Gmail</span>
        
      </a></li>
      <li class="w3-padding-16"><a style="text-decoration:none" href="www.linkedin.com/in/nikunj-shah-287263129">
        
        <span class="w3-large"><i class="fa fa-linkedin-square" style="font-size:24px"></i> LinkedIn</span>
        
      </a></li> 
      <li class="w3-padding-16"><a style="text-decoration:none" href="https://github.com/nikunjmshah">
        
        <span class="w3-large"><i class="fa fa-github" style="font-size:24px" ></i> Github</span>
      </a></li>   
      <li class="w3-padding-16 w3-hide-medium w3-hide-small"><a style="text-decoration:none" href="https://www.facebook.com/profile.php?id=100009831463915">
        
        <span class="w3-large"><i class="fa fa-facebook-square" style="font-size:24px"></i> Facebook </span>
        
      </a></li>  
    </ul>
  </div>
  <hr> 
 
  
  
<!-- END Introduction Menu -->
</div>





<!-- END GRID -->
</div><br>

<!-- END w3-content -->
</div>

<!-- Footer -->

<footer class="w3-container w3-dark-grey w3-padding-32 w3-margin-top">
  <button class="w3-button w3-black w3-padding-large w3-margin-bottom">Suggestions?</button>
  
</footer>
</body>
</html>
